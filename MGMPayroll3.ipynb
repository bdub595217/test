{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "#Download the data and create a dataframe\n",
    "client = Socrata(\"data.montgomeryal.gov\", None)\n",
    "results = client.get(\"pjb8-sd6v\", limit=10000)\n",
    "raw_data = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of raw_data and only the columns we need\n",
    "data = raw_data.drop(['annualsalaryytd', 'name', 'otherpayamt', 'otherpaydesc', 'overtimeamt', 'hire_date', 'position_title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2405 entries, 0 to 2404\n",
      "Data columns (total 5 columns):\n",
      "annual_salary    2405 non-null float64\n",
      "department       2405 non-null object\n",
      "grade            2405 non-null object\n",
      "positiontype     2405 non-null object\n",
      "step             2405 non-null float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 94.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Convert necessary columns to numeric\n",
    "num_cols = pd.Index(['annual_salary', 'step'])\n",
    "data[num_cols] = data[num_cols].astype(np.float64)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the CategoricalEncoder class, copied from PR #9151.\n",
    "# CategoricalEncoder is not to be released until version 0.20 of sklearn\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "num_attribs = [\"step\"]\n",
    "cat_attribs = [\"department\", \"grade\", \"positiontype\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the necessary columns\n",
    "X = data.drop(['annual_salary'], axis=1)\n",
    "y = data[[\"annual_salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = full_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model to the training data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522.0273322102196"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the mean squared error of the training data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_mse = mean_squared_error(y_train, lin_reg.predict(X_train))\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850125345756916.5"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now check the mean squared error of the training data.  What the!?\n",
    "predictions = lin_reg.predict(X_test)\n",
    "lin_mse = mean_squared_error(y_test, predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[             46980],\n",
       "       [             42930],\n",
       "       [             51370],\n",
       "       [             38507],\n",
       "       [             32365],\n",
       "       [             40713],\n",
       "       [             57423],\n",
       "       [             34594],\n",
       "       [             40924],\n",
       "       [             65700],\n",
       "       [             18031],\n",
       "       [             56135],\n",
       "       [             32365],\n",
       "       [             34581],\n",
       "       [             39605],\n",
       "       [             39605],\n",
       "       [             31268],\n",
       "       [-12999067298472076],\n",
       "       [             19933],\n",
       "       [             45322],\n",
       "       [             35315],\n",
       "       [             32122],\n",
       "       [             40713],\n",
       "       [             38507],\n",
       "       [             36287],\n",
       "       [             52030],\n",
       "       [             56369],\n",
       "       [             34735],\n",
       "       [             41739],\n",
       "       [             31730],\n",
       "       [             39607],\n",
       "       [             39615],\n",
       "       [            129277],\n",
       "       [             39150],\n",
       "       [             41831],\n",
       "       [             50960],\n",
       "       [             32388],\n",
       "       [             47133],\n",
       "       [             58733],\n",
       "       [             46025],\n",
       "       [              6321],\n",
       "       [             45306],\n",
       "       [             24655],\n",
       "       [             41821],\n",
       "       [             49349],\n",
       "       [             42524],\n",
       "       [             40713],\n",
       "       [             40873],\n",
       "       [             52819],\n",
       "       [             23175],\n",
       "       [             54549],\n",
       "       [             44917],\n",
       "       [             22402],\n",
       "       [             40713],\n",
       "       [             35394],\n",
       "       [             24543],\n",
       "       [             58585],\n",
       "       [              7429],\n",
       "       [             26993],\n",
       "       [             38060],\n",
       "       [             40723],\n",
       "       [             23175],\n",
       "       [             23770],\n",
       "       [             42939],\n",
       "       [             26319],\n",
       "       [             41831],\n",
       "       [             45156],\n",
       "       [             35964],\n",
       "       [             30841],\n",
       "       [             12970],\n",
       "       [             23973],\n",
       "       [             33571],\n",
       "       [             36638],\n",
       "       [             37798],\n",
       "       [             46980],\n",
       "       [             42930],\n",
       "       [             23175],\n",
       "       [             52478],\n",
       "       [             61586],\n",
       "       [             38507],\n",
       "       [             51711],\n",
       "       [             31676],\n",
       "       [             76065],\n",
       "       [             82652],\n",
       "       [             26142],\n",
       "       [             20192],\n",
       "       [             71729],\n",
       "       [             54780],\n",
       "       [             42341],\n",
       "       [             44624],\n",
       "       [             34874],\n",
       "       [             40713],\n",
       "       [             53533],\n",
       "       [             32040],\n",
       "       [             39615],\n",
       "       [             42930],\n",
       "       [             28792],\n",
       "       [             32560],\n",
       "       [             38715],\n",
       "       [             40359],\n",
       "       [             43525],\n",
       "       [             57423],\n",
       "       [             73724],\n",
       "       [             22143],\n",
       "       [             60283],\n",
       "       [             53138],\n",
       "       [             42930],\n",
       "       [             42548],\n",
       "       [             51301],\n",
       "       [             50116],\n",
       "       [             52846],\n",
       "       [             26582],\n",
       "       [             26993],\n",
       "       [             77173],\n",
       "       [             31730],\n",
       "       [             24543],\n",
       "       [            108460],\n",
       "       [             49349],\n",
       "       [             36199],\n",
       "       [             40118],\n",
       "       [             42939],\n",
       "       [             54780],\n",
       "       [             25473],\n",
       "       [             34735],\n",
       "       [              6321],\n",
       "       [             43809],\n",
       "       [             50305],\n",
       "       [             41821],\n",
       "       [             60378],\n",
       "       [             23175],\n",
       "       [             39185],\n",
       "       [             62019],\n",
       "       [             22143],\n",
       "       [             39605],\n",
       "       [             60911],\n",
       "       [             51370],\n",
       "       [             16168],\n",
       "       [             46025],\n",
       "       [             20192],\n",
       "       [             39605],\n",
       "       [             45156],\n",
       "       [             46025],\n",
       "       [             39605],\n",
       "       [             47210],\n",
       "       [             76162],\n",
       "       [             31730],\n",
       "       [             52409],\n",
       "       [             55272],\n",
       "       [ -3110225316444274],\n",
       "       [             23957],\n",
       "       [             55211],\n",
       "       [             28154],\n",
       "       [             41831],\n",
       "       [             25884],\n",
       "       [             49197],\n",
       "       [             38325],\n",
       "       [             39605],\n",
       "       [             24284],\n",
       "       [             40713],\n",
       "       [             52478],\n",
       "       [             50116],\n",
       "       [             32172],\n",
       "       [             46625],\n",
       "       [             32443],\n",
       "       [             15526],\n",
       "       [             22143],\n",
       "       [             22402],\n",
       "       [             42930],\n",
       "       [             22402],\n",
       "       [             40713],\n",
       "       [             49349],\n",
       "       [             39605],\n",
       "       [             31739],\n",
       "       [             37651],\n",
       "       [             52674],\n",
       "       [             28186],\n",
       "       [             40713],\n",
       "       [             12970],\n",
       "       [             31335],\n",
       "       [             36709],\n",
       "       [             29460],\n",
       "       [             39605],\n",
       "       [             24284],\n",
       "       [             56369],\n",
       "       [             39615],\n",
       "       [             38236],\n",
       "       [             40713],\n",
       "       [             42437],\n",
       "       [             41141],\n",
       "       [             30760],\n",
       "       [             48241],\n",
       "       [             46025],\n",
       "       [             39605],\n",
       "       [             39168],\n",
       "       [             33688],\n",
       "       [             73457],\n",
       "       [             25870],\n",
       "       [             24543],\n",
       "       [             22143],\n",
       "       [             40713],\n",
       "       [             50525],\n",
       "       [             31355],\n",
       "       [             24778],\n",
       "       [             35601],\n",
       "       [             41821],\n",
       "       [             23175],\n",
       "       [            106574],\n",
       "       [             33148],\n",
       "       [             56797],\n",
       "       [             39236],\n",
       "       [             52409],\n",
       "       [             37918],\n",
       "       [             33659],\n",
       "       [             38236],\n",
       "       [             45941],\n",
       "       [             52478],\n",
       "       [             53533],\n",
       "       [             40723],\n",
       "       [             51301],\n",
       "       [             53138],\n",
       "       [             31739],\n",
       "       [             28830],\n",
       "       [             46025],\n",
       "       [             40135],\n",
       "       [             25027],\n",
       "       [             41821],\n",
       "       [             32258],\n",
       "       [             48241],\n",
       "       [             47948],\n",
       "       [             38236],\n",
       "       [             43809],\n",
       "       [             74259],\n",
       "       [             51566],\n",
       "       [             46980],\n",
       "       [             30060],\n",
       "       [             48089],\n",
       "       [             32519],\n",
       "       [             38060],\n",
       "       [             32958],\n",
       "       [             73120],\n",
       "       [             35860],\n",
       "       [             39605],\n",
       "       [             23175],\n",
       "       [             44764],\n",
       "       [             23175],\n",
       "       [             54694],\n",
       "       [             12970],\n",
       "       [             40713],\n",
       "       [             55027],\n",
       "       [             14078],\n",
       "       [-12999067298484070],\n",
       "       [             51225],\n",
       "       [             41821],\n",
       "       [             43090],\n",
       "       [             39884],\n",
       "       [             24284],\n",
       "       [             39605],\n",
       "       [             38345],\n",
       "       [             39168],\n",
       "       [             42930],\n",
       "       [             40713],\n",
       "       [             27776],\n",
       "       [             67127],\n",
       "       [             56135],\n",
       "       [             22402],\n",
       "       [             48089],\n",
       "       [             52478],\n",
       "       [               780],\n",
       "       [             31335],\n",
       "       [             43809],\n",
       "       [             25421],\n",
       "       [             44517],\n",
       "       [             55206],\n",
       "       [             75431],\n",
       "       [             33433],\n",
       "       [             40713],\n",
       "       [             31335],\n",
       "       [             34224],\n",
       "       [             39615],\n",
       "       [             39605],\n",
       "       [             23714],\n",
       "       [             40713],\n",
       "       [             22402],\n",
       "       [             24284],\n",
       "       [             42931],\n",
       "       [             52030],\n",
       "       [             42847],\n",
       "       [             34977],\n",
       "       [             26065],\n",
       "       [             39605],\n",
       "       [             61166],\n",
       "       [             38236],\n",
       "       [             28186],\n",
       "       [             42548],\n",
       "       [             45156],\n",
       "       [             33627],\n",
       "       [             30060],\n",
       "       [             43499],\n",
       "       [             23175],\n",
       "       [             32587],\n",
       "       [             34224],\n",
       "       [             35844],\n",
       "       [             38507],\n",
       "       [             46733],\n",
       "       [              8538],\n",
       "       [             44764],\n",
       "       [             27722],\n",
       "       [             44086],\n",
       "       [             41831],\n",
       "       [             35684],\n",
       "       [             44767],\n",
       "       [             29294],\n",
       "       [             37819],\n",
       "       [             44362],\n",
       "       [             44994],\n",
       "       [             35046],\n",
       "       [             26065],\n",
       "       [              8410],\n",
       "       [             47133],\n",
       "       [             44395],\n",
       "       [             55355],\n",
       "       [             22402],\n",
       "       [             37582],\n",
       "       [             53533],\n",
       "       [             18031],\n",
       "       [             33231],\n",
       "       [             30622],\n",
       "       [             53138],\n",
       "       [             46131],\n",
       "       [             49008],\n",
       "       [             23714],\n",
       "       [             40713],\n",
       "       [             51301],\n",
       "       [             35739],\n",
       "       [             38715],\n",
       "       [             46980],\n",
       "       [             38507],\n",
       "       [             44653],\n",
       "       [             48241],\n",
       "       [              -327],\n",
       "       [             51301],\n",
       "       [             28297],\n",
       "       [             51370],\n",
       "       [             54098],\n",
       "       [             50193],\n",
       "       [             47635],\n",
       "       [             26500],\n",
       "       [             74140],\n",
       "       [             48480],\n",
       "       [             37565],\n",
       "       [             14078],\n",
       "       [             42128],\n",
       "       [             39816],\n",
       "       [             55734],\n",
       "       [             52409],\n",
       "       [             52478],\n",
       "       [             28347],\n",
       "       [             39075],\n",
       "       [             20192],\n",
       "       [             34541],\n",
       "       [             47372],\n",
       "       [             29840],\n",
       "       [             39605],\n",
       "       [             44653],\n",
       "       [             38128],\n",
       "       [             34735],\n",
       "       [             44038],\n",
       "       [             50305],\n",
       "       [             41579],\n",
       "       [             36319],\n",
       "       [             53517],\n",
       "       [             46792],\n",
       "       [             55206],\n",
       "       [             22402],\n",
       "       [             37984],\n",
       "       [             39605],\n",
       "       [             19933],\n",
       "       [             39615],\n",
       "       [             49737],\n",
       "       [             23175],\n",
       "       [             33644],\n",
       "       [             46980],\n",
       "       [             12970],\n",
       "       [             23808],\n",
       "       [             37162],\n",
       "       [             36757],\n",
       "       [             32040],\n",
       "       [             38236],\n",
       "       [             39615],\n",
       "       [             51225],\n",
       "       [             44917],\n",
       "       [             55657],\n",
       "       [             40723],\n",
       "       [             40723],\n",
       "       [             48430],\n",
       "       [             45156],\n",
       "       [             44038],\n",
       "       [             41821],\n",
       "       [             41107],\n",
       "       [             39605],\n",
       "       [             45869],\n",
       "       [             29849],\n",
       "       [             23510],\n",
       "       [             61391],\n",
       "       [             44917],\n",
       "       [             46980],\n",
       "       [             31014],\n",
       "       [             24762],\n",
       "       [             41831],\n",
       "       [             33953],\n",
       "       [             84930],\n",
       "       [             28952],\n",
       "       [             22143],\n",
       "       [             11475],\n",
       "       [             45146],\n",
       "       [             42545],\n",
       "       [             67917],\n",
       "       [             31676],\n",
       "       [             45211],\n",
       "       [             49419],\n",
       "       [             34420],\n",
       "       [             42930],\n",
       "       [             29906],\n",
       "       [             49085],\n",
       "       [             51225],\n",
       "       [              1889],\n",
       "       [             53517],\n",
       "       [             39605],\n",
       "       [             32365],\n",
       "       [             29514],\n",
       "       [             44764],\n",
       "       [             39168],\n",
       "       [             46025],\n",
       "       [             42219],\n",
       "       [             42177],\n",
       "       [             23175],\n",
       "       [             58105],\n",
       "       [             38166],\n",
       "       [             34224],\n",
       "       [             18824],\n",
       "       [             52030],\n",
       "       [             18031],\n",
       "       [             41821],\n",
       "       [             38507],\n",
       "       [             65966],\n",
       "       [             21679],\n",
       "       [             41507],\n",
       "       [             23175],\n",
       "       [             47133],\n",
       "       [             43632],\n",
       "       [             40723],\n",
       "       [             68130],\n",
       "       [             35677],\n",
       "       [             23175],\n",
       "       [             63400],\n",
       "       [             39615],\n",
       "       [             67832],\n",
       "       [             47868],\n",
       "       [             52333],\n",
       "       [             33163],\n",
       "       [             37582],\n",
       "       [             25027],\n",
       "       [             52333],\n",
       "       [             28154],\n",
       "       [             42968],\n",
       "       [             68668],\n",
       "       [             10394],\n",
       "       [             44038],\n",
       "       [             38236],\n",
       "       [             35055],\n",
       "       [             28347],\n",
       "       [             48505],\n",
       "       [             34735],\n",
       "       [             25543],\n",
       "       [             24778],\n",
       "       [             40976],\n",
       "       [             36808],\n",
       "       [             37817],\n",
       "       [             38060],\n",
       "       [             22143],\n",
       "       [             37582]], dtype=int64)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 18th prediction is way off\n",
    "np.int64(predictions[:,:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>annual_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2179</td>\n",
       "      <td>46946.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1525</td>\n",
       "      <td>43117.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1377</td>\n",
       "      <td>50968.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2372</td>\n",
       "      <td>38316.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125</td>\n",
       "      <td>32318.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1697</td>\n",
       "      <td>40676.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1973</td>\n",
       "      <td>57911.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1027</td>\n",
       "      <td>34567.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>929</td>\n",
       "      <td>40894.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>210</td>\n",
       "      <td>63243.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1517</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1724</td>\n",
       "      <td>54436.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>927</td>\n",
       "      <td>32318.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2028</td>\n",
       "      <td>34599.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1908</td>\n",
       "      <td>39456.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>56</td>\n",
       "      <td>39456.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1938</td>\n",
       "      <td>29955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>218</td>\n",
       "      <td>50251.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>632</td>\n",
       "      <td>21363.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>387</td>\n",
       "      <td>44944.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1146</td>\n",
       "      <td>33696.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>790</td>\n",
       "      <td>29373.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1503</td>\n",
       "      <td>40676.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1738</td>\n",
       "      <td>38316.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1456</td>\n",
       "      <td>35308.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2327</td>\n",
       "      <td>50968.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>575</td>\n",
       "      <td>56173.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1592</td>\n",
       "      <td>34386.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1178</td>\n",
       "      <td>41332.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1222</td>\n",
       "      <td>31406.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>472</td>\n",
       "      <td>39161.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>432</td>\n",
       "      <td>39500.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>741</td>\n",
       "      <td>144948.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>979</td>\n",
       "      <td>39010.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>486</td>\n",
       "      <td>41871.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1624</td>\n",
       "      <td>50251.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1241</td>\n",
       "      <td>32318.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>649</td>\n",
       "      <td>47049.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  annual_salary\n",
       "0    2179       46946.39\n",
       "1    1525       43117.57\n",
       "2    1377       50968.74\n",
       "3    2372       38316.10\n",
       "4    1125       32318.00\n",
       "5    1697       40676.69\n",
       "6    1973       57911.24\n",
       "7    1027       34567.10\n",
       "8     929       40894.26\n",
       "9     210       63243.98\n",
       "10   1517       18000.00\n",
       "11   1724       54436.30\n",
       "12    927       32318.00\n",
       "13   2028       34599.34\n",
       "14   1908       39456.35\n",
       "15     56       39456.35\n",
       "16   1938       29955.95\n",
       "17    218       50251.76\n",
       "18    632       21363.06\n",
       "19    387       44944.22\n",
       "20   1146       33696.00\n",
       "21    790       29373.97\n",
       "22   1503       40676.69\n",
       "23   1738       38316.10\n",
       "24   1456       35308.00\n",
       "25   2327       50968.94\n",
       "26    575       56173.94\n",
       "27   1592       34386.56\n",
       "28   1178       41332.30\n",
       "29   1222       31406.75\n",
       "30    472       39161.82\n",
       "31    432       39500.99\n",
       "32    741      144948.54\n",
       "33    979       39010.40\n",
       "34    486       41871.65\n",
       "35   1624       50251.76\n",
       "36   1241       32318.00\n",
       "37    649       47049.60"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first 18 y_test values\n",
    "y_test[:38].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218    50251.76\n",
       "Name: annual_salary, dtype: float64"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 18th prediction should be closer to this number\n",
    "y_test.annual_salary[17:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annual_salary               50251.8\n",
       "department       CITY INVESTIGATION\n",
       "grade                           A08\n",
       "positiontype              Full Time\n",
       "step                              4\n",
       "Name: 218, dtype: object"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The data for this record [218] doesn't look unusual\n",
    "data.iloc[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.29990673e+16  -3.11022532e+15  -1.29990673e+16  -3.27500000e+02]\n",
      "Length of wrong predictions:  4\n"
     ]
    }
   ],
   "source": [
    "#All the predictions that are wrong\n",
    "print(np.append(predictions[predictions < 0], predictions[predictions > 150000]))\n",
    "print(\"Length of wrong predictions: \", len(np.append(predictions[predictions < 0], predictions[predictions > 150000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09457946  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          1.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          1.          0.          0.\n",
      "   0.        ]\n",
      " [-0.09457946  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0977265   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.90856759  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 1.41014111  0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 1.66092787  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [-0.09457946  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          1.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [-0.34536622  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 1.15935435  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[18:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[218, 629, 1286]"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of the indexes of the wrong predictions\n",
    "find = list(np.array(y_test.index[((y_test.annual_salary - predictions[:,0]) < -10000) | ((y_test.annual_salary - predictions[:,0]) > 150000)]))\n",
    "find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annual_salary</th>\n",
       "      <th>department</th>\n",
       "      <th>grade</th>\n",
       "      <th>positiontype</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>50251.76</td>\n",
       "      <td>CITY INVESTIGATION</td>\n",
       "      <td>A08</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>95000.08</td>\n",
       "      <td>MAYOR</td>\n",
       "      <td>888</td>\n",
       "      <td>Full Time Exempt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>39161.82</td>\n",
       "      <td>CITY INVESTIGATION</td>\n",
       "      <td>A05</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      annual_salary          department grade      positiontype  step\n",
       "218        50251.76  CITY INVESTIGATION   A08         Full Time   4.0\n",
       "629        95000.08               MAYOR   888  Full Time Exempt   1.0\n",
       "1286       39161.82  CITY INVESTIGATION   A05         Full Time   7.0"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter the original data to view records\n",
    "data.iloc[find]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['position_type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-638-3d4a645e2a80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'department'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2000\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2002\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2003\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['position_type'] not in index\""
     ]
    }
   ],
   "source": [
    "data[['department', 'position_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[17:18]!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[16:17]!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[17:18]!=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
